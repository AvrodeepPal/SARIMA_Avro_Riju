# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_percentage_error
import warnings

warnings.filterwarnings('ignore')

# Load the data
file_path = 'Final.csv'
data = pd.read_csv(file_path)

# Display the column names
print("Column names in the dataset:")
print(data.columns)

# Melt the dataset to long format
date_columns = data.columns[3:]  # Columns starting from 'Apr-18' to 'May-21'
data_long = pd.melt(
    data,
    id_vars=['Warehouse id', 'Region', 'SKU id'],
    value_vars=date_columns,
    var_name='Date',
    value_name='Sales'
)

# Rename columns for clarity
data_long.rename(columns={'Warehouse id': 'Warehouse', 'SKU id': 'SKU'}, inplace=True)

# Convert the 'Date' column to a proper datetime format
data_long['Date'] = pd.to_datetime(data_long['Date'], format='%b-%y')

# Sort the data
data_long = data_long.sort_values(by=['Warehouse', 'SKU', 'Date'])

# Fill missing sales with 0
data_long['Sales'] = data_long['Sales'].fillna(0)

# Aggregating data by SKU and warehouse
agg_data = data_long.groupby(['Date', 'Warehouse', 'SKU', 'Region'])['Sales'].sum().reset_index()

# Plot Overall Sales Trends
plt.figure(figsize=(12, 6))
sns.lineplot(data=data_long, x='Date', y='Sales', hue='Warehouse', ci=None, marker='o')
plt.title('Overall Sales Trends by Warehouse', fontsize=16)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Sales', fontsize=12)
plt.legend(title='Warehouse', loc='upper left')
plt.grid()
plt.show()

# Seasonal Decomposition for an Example SKU and Warehouse
example_sku = agg_data['SKU'].iloc[0]
example_warehouse = agg_data['Warehouse'].iloc[0]
example_data = agg_data[(agg_data['SKU'] == example_sku) & (agg_data['Warehouse'] == example_warehouse)]
time_series = example_data.set_index('Date')['Sales']

# Ensure sufficient data points for decomposition
if len(time_series) >= 24:
    decomposition = seasonal_decompose(time_series, model='additive', period=12)
    decomposition.plot()
    plt.suptitle(f'Seasonal Decomposition for SKU: {example_sku}, Warehouse: {example_warehouse}', fontsize=16)
    plt.show()

# Function to prepare data for forecasting
def prepare_data(df, warehouse, sku):
    sku_data = df[(df['Warehouse'] == warehouse) & (df['SKU'] == sku)].set_index('Date')['Sales']
    return sku_data.resample('M').sum()

# Forecasting for all SKU-Warehouse combinations
forecasts = []
errors = []

for warehouse in agg_data['Warehouse'].unique():
    for sku in agg_data['SKU'].unique():
        time_series = prepare_data(agg_data, warehouse, sku)

        # Handle missing data by filling with 0
        time_series = time_series.fillna(0)

        # Skip empty time series
        if time_series.empty:
            print(f"No data available for SKU: {sku}, Warehouse: {warehouse}. Skipping forecast.")
            forecasts.append({'SKU': sku, 'Warehouse': warehouse, 'Forecasted Sales': np.nan})
            errors.append({'SKU': sku, 'Warehouse': warehouse, 'MAPE': np.nan})
            continue

        # Train-test split: Use data until May 2021 for training
        train_data = time_series[:'2021-05']

        # Ensure training data is not empty
        if train_data.empty:
            print(f"No training data available for SKU: {sku}, Warehouse: {warehouse}. Skipping forecast.")
            forecasts.append({'SKU': sku, 'Warehouse': warehouse, 'Forecasted Sales': np.nan})
            errors.append({'SKU': sku, 'Warehouse': warehouse, 'MAPE': np.nan})
            continue

        # Fit SARIMA model
        try:
            model = SARIMAX(train_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12), enforce_stationarity=False, enforce_invertibility=False)
            sarima_fit = model.fit(disp=False)

            # Forecast for June 2021
            forecast = sarima_fit.forecast(steps=1).iloc[0]
            forecasts.append({'SKU': sku, 'Warehouse': warehouse, 'Forecasted Sales': forecast})

            # Evaluate performance (train-test)
            predictions = sarima_fit.fittedvalues
            error = mean_absolute_percentage_error(train_data, predictions)
            errors.append({'SKU': sku, 'Warehouse': warehouse, 'MAPE': error})
        except Exception as e:
            print(f"Failed to forecast for SKU: {sku}, Warehouse: {warehouse}. Error: {e}")
            forecasts.append({'SKU': sku, 'Warehouse': warehouse, 'Forecasted Sales': np.nan})
            errors.append({'SKU': sku, 'Warehouse': warehouse, 'MAPE': np.nan})

# Save forecast results in the desired format
forecast_df = pd.DataFrame(forecasts)
forecast_df = forecast_df.merge(data[['Warehouse id', 'Region', 'SKU id']], 
                                 how='left', 
                                 left_on=['Warehouse', 'SKU'], 
                                 right_on=['Warehouse id', 'SKU id']).drop_duplicates()

forecast_df.rename(columns={'Forecasted Sales': 'Jun-21'}, inplace=True)
forecast_df = forecast_df[['Warehouse id', 'Region', 'SKU id', 'Jun-21']]

# Sort by numeric part of SKU id
forecast_df['SKU_num'] = forecast_df['SKU id'].str.extract(r'(\d+)$').astype(int)  # Extract numeric part of SKU id
forecast_df = forecast_df.sort_values(by=['SKU_num', 'Warehouse id']).drop(columns=['SKU_num'])

# Save to CSV
forecast_df.to_csv('Submission.csv', index=False)
print("\nForecasts for June 2021 saved to 'Submission.csv'")

# Save error metrics
error_df = pd.DataFrame(errors)
error_df = error_df.merge(data[['Warehouse id', 'Region', 'SKU id']], 
                          how='left', 
                          left_on=['Warehouse', 'SKU'], 
                          right_on=['Warehouse id', 'SKU id']).drop_duplicates()

error_df.rename(columns={'MAPE': 'Error (MAPE)'}, inplace=True)
error_df = error_df[['Warehouse id', 'Region', 'SKU id', 'Error (MAPE)']]
error_df.to_csv('Model_Errors.csv', index=False)
print("\nModel errors saved to 'Model_Errors.csv'")
